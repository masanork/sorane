# 技術の民主化と今昔

足し算や引き算は親父が書いたNEC PC-6001のBASICプログラムで教わった。PC-9801天下の時代にパソコンは高くなって、しばらくはワープロしか買えなくなった。MSXが数万円の時代に、ワープロは十数万円でAMD186なんかが載っていて、PC-9801でモニター、プリンター、ワープロ、表計算と一式揃えると50万、Macで揃えると100万じゃきかず200万の大台に乗ることもあっただろう。MacやiPhoneが高くなったというけれども、NeXTの末裔が十数万円で手に入るのだから安いものだ。

当然ながらコンパイラも買うものだった。そこに風穴を開けたのはGCCであり、LinuxやFreeBSDをはじめとしたPC UNIXでありJavaだった。ハイパーテキストをつくろうにもヘルプコンパイラの入手方法が分からない時代に、MOSAICやNetscapeではHTMLを書くだけでハイパーテキストを表示してくれた。大学時代Windows CEでセルフ開発しようとしたら、有償のWZ Editorで使われていたC言語ライクなマクロかPythonくらいしか選択肢がなかった。

TensorFlowやPyTorchのエコシステムも、そういったオープンなエコシステムの延長線上にあるし、ChatGPTはプログラミングのプロによるコーチングでさえもコモディティ化しようとしている。一方で昨今のNVIDIA GPUの取り合いやら、計算量で閾値を超えると知恵が生まれる的な世界は、世の中が再びワークステーションを買える者、CRAYのようなスパコンをあてがわれた者のみが技術革新に参加できる1980年代に逆戻りしているのだろうか。

その世界でさえ汎用機しかなかった1960年代をミニコンとマイコンが1970年代に塗り替えて、パソコンのエコシステムが広がった最中の些細な階級闘争に過ぎず、あの時代から計算機を触っていたこと自体が、十分に恵まれた世界だったんだろうし、1980年代にスパコンを触ろうとしたら大学や電機メーカーの研究所に入るなり、リクルートでそういった部署に配属される必要があったところ、ちょっとPyTorchを触ろうとGoogle Colabを弄る分には無料でもそれなりのことができて、月額数千円でそこそこのことができるんだから、やはり1980年代と比べたら十分に可能性が開かれているようにも感じる。それ以前のバリアーがむしろ論点になるんだろうとも感じる。
